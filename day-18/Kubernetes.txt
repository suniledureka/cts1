Container Orchestration using Kubernetes
========================================

Music Orchestration.

People should know when to stop or when to start again or when to pass the playing.

So to give such commands, we'll be having a person with the name conductor in the center of the stage.

Person who is controlling all these in between, we call him as an conductor.

==== What is container orchestration? ====

As of now, we are aware that we are trying to build lot many microservices , first- we are building spring boot applications and the same application we are trying to package it as a Docker image and post that- with the help of Docker server we are converting these images into running containers so that we can access all our microservice applications.


As of now, inside our use-cases we built maximum of 6 or 7 microservices, but in real production applications,
sometimes you may have more than 100 microservices. That means you may have more than 100 different containers running inside your production server.

So whenever you have such kind of large number of containers inside your organizations, we need a component
that is going to take care of container orchestration.

So just like how conductor inside the music orchestra is controlling the lot many musicians - similarly, we need a component which can control our containers based upon the requirements that we have.


==== Why should we take care of container orchestration inside our microservice network? =====

Challenge #1
1. Automating the deployments, Rollouts & Rollbacks
 - How do we automate the deployment of the containers into a complex cluster environment and perform rollout of new versions of the containers without downtime along with an option of automatic rollback in case of any rollbacks?

The very first question is how we are going to automate the deployments and rollouts and rollbacks. Inside
microservices we should try to automate as much as possible - because inside microservices we are going to deal hundreds of applications, whereas with monolithic, there is only one application; so even without automation, you can easily survive inside the monolithic application; whereas when you try to move to the microservices, you should try to automate each and every task, like doing deployments, rollouts and rollbacks.

What is the meaning of rollout and rollbacks?
--> whenever you try to make some changes or whenever you try to build a new version of your microservices, you need to push them or deploy them into the production server. When you try to do that deployment- bigger organizations, they cannot afford a downtime. So in such scenarios they will go with an option of rollout. So as part of this rollout, they are going to replace the containers one by one with the latest Docker image.

Think like for "accounts" microservice - you have three Docker containers running inside the production. With the help of this rollout, we can first try to create a new container with the latest Docker image. Once the new container is available, then only we can terminate the previous running containers.
So this will avoid any downtime that organizations may face while doing the deployment.

And on the similar lines, think like when you roll out a new feature or a new Docker image into your containers, you face some issue. There is a bug identified in the production server. In such scenarios, you should also have a flexibility of automatic rollback to the previous running version of the Docker image in case of any issues.

So how we are going to achieve all this automatically.

So that's a very first challenge that we will try to solve with the help of container orchestration.

Challenge#2 
Making sure our services are self-healing?
How are you going to make your microservices as self-healing capable?

That means if one of the microservice container is not responding properly or it is responding very slowly, you need to have someone to regularly perform the health check on these running containers and take an automatic decision by killing the containers, which is not responding properly and replacing them with a new containers.


Challenge# 3
Autoscaling our services
How are you going to make your services auto scaling?

So what is scaling- when are we seeing the increase of the traffic that is coming towards our application we can try to onboard more number of containers or more number of servers.

But doing that manually for each and every microservice is going to be an impossible task. So that's why we need to look for options on how to do auto scaling of our microservices. Based upon the CPU utilization or other metrics of the running container, we should be able to automatically take a decision whether to scale up or scale down the running containers.

For example, let's take an example of Netflix. Usually there will be a lot of traffic to the Netflix on the Friday night, on Saturday and Sunday. So during these days the Netflix should automatically scale their number of microservice instances or applications so that they can stream the content to their customers without any issues.

And similarly, there can be sudden spike in traffic sometimes, maybe due to some long weekend or due to some holiday in between of the week. So all such scenarios Netflix should be able to automatically handle that by increasing more number of microservices at runtime with the help of auto scaling.

Very similarly, we should also able to perform auto scaling on our microservices.

So how we are going to do that, that's a challenge here.

So to answer all these challenges or questions, we are going to use one of the container orchestration product - Kubernetes.

As a microservice developer, it is very important for you to understand what is Kubernetes is capable of, what is Docker is capable of.

=====================
What is a Kubernetes?
=====================

Kubernetes is an open source container orchestration platform that is capable of automating the deployments, rollouts, scaling and managing all your containerized applications.

This Kubernetes is originally developed by Google, and later on they open sourced it and right now it is being maintained by the Cloud Native Computing Foundation (CNCF).



Kubernetes as an container orchestration product.

Kubernetes is an open source system for automating the deployments, scaling and managing the containerized applications.

It is the most famous orchestration platform that is available as of now in the market and one more advantage of Kubernetes is it is cloud neutral.

If you have Kubernetes cluster set up inside your local system or inside AWS, GCP, Azure, regardless of wherever you set up, the Kubernetes concepts are going to be similar. That's why we can call Kubernetes as a cloud neutral.



Kubernetes was developed and open sourced by Google; somewhere in 2015, Google decided to open source their one of the internal project that they have developed over 15 years. With these Kubernetes concepts only behind the scenes, Google tried to run their majority of the products like YouTube, Google Photos, Gmail.

Kubernetes provides you with a framework to run distributed systems like cloud native applications or microservices  resiliently. It is capable of taking care of automated scaling and handling the fail over for your application, providing deployment patterns that will assure zero downtime for your applications.

Kubernetes also provides you with:
 1. Service discovery and load balancing - we can get rid of Eureka Server and we can hand over the load balancing to the Kubernetes
 2. Container & storage orchestration - with the help of Kubernetes, we can control any number of containers along with their storage requirements.
 3. Automatic rollouts and rollbacks - Kubernetes is also capable of automated rollouts and rollbacks
 4. Self-healing
 5. Secret and configuration management





The word Kubernetes originates from Greek.

Inside Greek language, the meaning of Kubernetes is helmsman (a person who steers a ship or boat.) or pilot who is going to handle the ship. That's why we have this logo for Kubernetes.

======================
   SUMMARY
======================
-> Kubernetes is a container management tool
-> K8s is an open source orchestration platform
-> K8s is used to manage our containers
-> K8s provides a framework to handle containers related tasks (deployment, scaling, load balancing etc)
-> Kubernetes is an open source container orchestration engine or container management tool, it automates deploying, scaling and managing containerized application

-> K8s is developed by Google and handed over to CNCF (Cloud Native Computing Foundation )
-> The name Kubernetes originates from Greek, meaning 'helmsman' or 'pilot'.

-> Kubernetes will manage our containers at runtime

-> Kubernetes is a container management tool/ Container Orchestration Engine - orchestrates container activity
K8s provides a framework for managing the complex task of deploying, scaling, and operating applications in containers

Container - Docker
Management - deploying, scheduling, scaling, load balancing Tool (on behalf of you K8s will take care of all)

K8s is used for managing the docker containers
K8s is used for application deployments in the real time



K8s Advantages
1. Orchestration - management
2. Self Healing - if container damaged it will create new one
3. Load Balancing - distribute requests to all running containers
4. Auto Scaling - scale up and scale down containers on demand basis




================================
Kubernetes Internal Architecture
================================
Slide - 19

What are the components inside Kubernetes that are responsible to do the magic of automated deployments, rollouts, scaling and many other advantages provided by the Kubernetes?

Whenever we say Kubernetes, we are talking about a cluster.

So what is a cluster?

Inside cluster will have set of servers or virtual machines which are going to work together to deliver a desired output to the end user.

So very similarly inside Kubernetes cluster also we will have various servers or virtual machines which are going to work together to make our microservices always running properly without any issues.

Here you may have a question like why should we go with a complex setup of Kubernetes cluster? Can't we deploy all our microservices with the help of Docker compose alone?

We cannot deploy all our containers with the help of Docker compose. Whenever you are using Docker compose, you are going to deploy all your containers inside a single server. 
But in real production -applications you may have hundreds of microservices. We cannot deploy all our hundreds of microservices with the help of Docker compose in a single server. 
Instead, we want to build and multi distributed environment where we can deploy our microservices in various servers or  nodes inside a cluster.

On top of that, your Docker or Docker compose is not capable of automatic deployments, rollouts, scaling; so that's why we need to leverage Kubernetes, which is a container orchestration.

===> there is a lot of manual work involved, if you use only containers and docker inside your production environment.


Usually a Kubernetes cluster will have multiple nodes.

Whenever we are talking about a Kubernetes cluster, your Kubernetes cluster will have two types of nodes:
 1. Master Node / Cluster Plane/ Control Panel- responsible to controlling and maintaining your entire Kubernetes cluster
 2. Worker Node - responsible to handle the traffic that we get towards our microservices.


Inside your master node, there is going to be a very good component, which is "Kube API server".

Kube API server is going to expose some APIs using which anyone from outside Kubernetes cluster can interact with the master node and using the same API server only the master nodes and worker nodes - going to communicate with each other.

There are two approaches that we can follow whenever we want to interact with the Kubernetes cluster.

1. Using admin UI of the Kubernetes.
2. Using the kubectI CLI.

So from your CLI terminal you can execute some kubectl commands and these commands will be input to the Kube API server.

So with the help of YAML configurations you can always provide some instructions to your Kubernetes cluster saying that I want so-and-so microservice to be deployed with so and so replicas with so and so docker image. So all those details we can provide as an input details inside a YAML  configuration to the master node with the help of kubectl CLI or admin UI.

So these commands that we can give from outside of the Kubernetes cluster is going to be received by the Kube API server.

So once my Kube API server receives instructions through kubectl CLI or admin UI, it is going to read the instructions- What is the end user is trying to do, whether he wants to create a deployment or whether he wants to do some auto-scaling; So based upon the instructions that it understand it is going to give those instructions to the SCHEDULER.

===> What is the purpose of scheduler component? 
Scheduler will understand what are the requirements that it received from the Kube API server and based upon the requirements it is going to identify under which worker node it has to do a deployment. Suppose if my end user gave an instructions to the master node saying that I want to deploy my "accounts" to microservice.

In such scenario, the kube API server will give these instructions to the scheduler. Scheduler is a component that is responsible to identify under which worker node the deployment of "accounts" microservice has to be done. 
So behind the scenes, it will do a lot of calculations like which worker node has bandwidth, which worker nodes is super busy. So by considering all these calculations, it is going to identify one of the worker node present inside the Kubernetes cluster.


Once my scheduler identifies - under which worker node it has to do the accounts microservice deployment, it is going to give the same instructions back to the Kube API server and from Kube API server it will reach to the corresponding worker node about the deployment of the accounts microservice.

So now assume like your accounts microservice deployment is completed inside one of the worker node available, but the story is not going to end there. Maybe after few days or maybe after few hours, your container may have some problems.

So who is going to track that? So it is the responsibility of the controller manager to always track the containers and worker nodes available inside the cluster.

If any of the worker node or container is having some health issues, this controller manager is going to make sure it is bringing new worker nodes or new containers in the place of problematic containers are worker nodes.

In simple words, we can say controller manager always have an input value saying that this is the desired state of my Kubernetes cluster. Inside my Kubernetes cluster, I always want to make sure I have three instances of accounts microservice running always.

So my controller manager regularly keep health check of these three running instances of accounts microservice, If any of the worker node or any of the container has issues, it is going to match with that desired state; which means if one of the container is having some problems since my controller manager knows always three containers has to be in a good health status, it is going to kill the container which has some problems and in the same place it is going to bring the new container of accounts microservice.

This way it will always try to match with the desired state that it received from the end user with the actual state present inside the Kubernetes cluster.

Now, apart from controller manager, scheduler and Kube API server, we also have another important component, which is Etcd.

What is this etcd?
--> We can say this etcd as a brain of our Kubernetes cluster, because this etcd is going to act as a database or a storage system for your Kubernetes cluster -inside this etcd only all the information related to your Kubernetes cluster is going to stored as a key value pair.

For example, if my controller manager at any point has some questions like how many replicas it has to maintain about accounts microservice, it can always connect with the etcd to understand what is the desired state that we initially received from the end user.

And very similarly, the Kube API server when it receives the instructions from the end user with the help of CLI or admin UI, it is going to make an entry into the etcd so the same values can be referred by the controller manager scheduler during their day to day operations.

Now Worker Nodes --> inside worker node also we have very good number of components.

1. Kubelet.

Kubelet is an agent running inside all your worker nodes;  using these Kubelet only my master node is going to connect with the worker node and it is going to provide the instructions with the help of kube API server. 

All our microservices has to be deployed inside the worker node, so my master node has to give instructions to the worker node saying that please deploy this accounts microservice with a replica of three.

So all such instructions the worker nodes are going to receive with the help of Kubelet.

2. Container runtime.
So what is the purpose of container runtime?
since we are going to deploy all our microservices in the form of containers, we need to make sure there is some container runtime installed inside the worker node.

Most of the times the container runtime is going to be Docker because that is the most widely used container environments.

So when you try to set up Kubernetes cluster, all your worker nodes, they are going to have that Docker server installed inside them.

Now behind that container runtime, you can see we have Pod.

3. POD

Pod is a smallest deployment unit inside the Kubernetes- like worker node is going to be a jumbo server or a jumbo virtual machine.
We cannot deploy our containers directly into the worker nodes.

Instead, the Kubernetes is going to create a pod inside a worker node. And inside this pod only the actual containers of the microservices are going to be deployed.

So what is the reason to have the pods inside Kubernetes cluster?

Suppose if you are trying to deploy multiple microservices into your same worker node to provide that isolation from other microservices, we are going to have a concept of pods.

So inside these Pods only, the containers will be deployed.

Usually most of the times a pod will have a single container.

Suppose think like you are trying to deploy your accounts, microservice and card microservice and loans microservice; all these microservices are going to be deployed independently in different, different parts.
They will never be a scenario inside a single pod, multiple microservices will be deployed.

Always remember - inside a pod, only a specific microservice or a specific application only allowed.

we have mentioned two containers like container1 and container 2 in the picture.
So the reason sometimes you may deploy multiple containers inside a pod is sometimes your application container like accounts, loans or cards- it may need some helper container or it may need some utility container to perform its job.

So such helper containers we can deploy inside the same pod where we have the main container.

So this kind of deploying a helper container along with the main container inside a pod is called SIDECAR pattern.

Kubernetes will make sure there is only one container inside a pod which is related to a single microservice application.

1. we'll give instructions to the master node and the master node is going to work with the scheduler, controller manager and etcd and post that it is going to give instructions to the worker node through kubelet saying that I want so-and-so microservice to be deployed as a container.

The same will be deployed as a container into a Pod. But to expose your container to the outside world or to the other containers inside the same cluster, Kubernetes is going to use a component called Kube Proxy. With the help of Kube proxy only - all the containers available inside the worker nodes - they are going to expose themselves to the outside world or they can also restrict themselves that the communication they are going to accept only within the cluster.

This is very similar to the way how we work inside our projects. 
Think like there are ten developers who are going to do the actual work of development.

So to manage all these developers will have a manager.

So very similarly here also the master node is going to manage a set of worker nodes.

If you have large number of worker nodes, then obviously we need more number of master nodes.


================================
K8s Architecture Components
================================
1. Kube API Server will receive incoming requests and it will store into ETCD
2. ETCD is K8s cluster database
3. Scheduler will check pending tasks in ETCD and it will schedule those task in worker nodes
4. Scheduler will get available worker nodes information by using Kubelet

5. Kubelet is called as Worker node agent
6. Kube-Proxy provides network for cluster communication

7. POD is the smallest building block that we run in Kubernetes cluster. POD represents runtime instance of our application

In K8s, our project will be executed as a POD. Inside POD containers will be created

8. Controller Manager will monitor all K8s resources functionality





=======================================================
Setup a local Kubernetes cluster using Docker Desktop
=======================================================

We are going to set up a Kubernetes cluster inside the local system.

To set up a local Kubernetes cluster, we have various options:

1. installation with the name Minikube.

What is Minikube?

With the help of Minikube installation, we can set up a small Kubernetes cluster inside local system.

But there are some drawbacks of Minikube installation because few of the commands that we give for the Minikube will be different compared to the actual commands that we give to the Kubernetes cluster inside the production environment.




==== Deploy a local Kubernetes cluster with the help of Docker Desktop ===

Google "deploy on kubernetes with docker desktop"
https://docs.docker.com/desktop/features/kubernetes/


Install and turn on Kubernetes
 1. Open the Docker Desktop Dashboard and navigate to Settings.
 2. Select the Kubernetes tab.
 3. Toggle on Enable Kubernetes.
 4. Choose your cluster provisioning method.
 5. Select Apply to save the settings. [Apply & Restart]

--> the above steps will create a SINGLE NODE local K8s cluster

this local Kubernetes cluster is going to work very similar to production Kubernetes cluster.
So now we have Kubernetes local cluster available.

Step-2 -  we need to make sure Kubectl is set up inside our local system.

What is kubect - it is one of the approach to interact with the Kubernetes cluster. - One is with the UI, the other one is Kubectl CLI.

So to give instructions to our Kubernetes cluster, we need to make sure we have kubectl setup inside our local system.

By default this kubectl is going to be set up by your Docker desktop while it is trying to create the local Kubernetes cluster at this path inside your

Mac or if you are using Windows, this is the path where it is going to set up.

So try to run some kubectl command.

So kubectl is a command every time we need to use whenever we want to use some instructions to the Kubernetes cluster.

C:\Program Files\Docker\Docker\resources\bin

cmd> kubectl version
cmd> kubectl config get-contexts   --> list of contexts available in my local system - context is a type of isolated environment using which my client application can interact with the K8s cluster 

cmd> kubectl config get-clusters  -- lists the clusters running inside your local system

cmd> kubectl config use-context docker-desktop

cmd> kubectl get nodes  --- lists the number of nodes available inside your local system

The Kubernetes control plane serves as the "brain" of a Kubernetes cluster, managing and orchestrating the entire system. It is a collection of components that work together to maintain the desired state of the cluster, schedule workloads, and handle API requests.


=== Turn off Kubernetes ====


==== 2. Deploying the Kubernetes Dashboard UI  =====

Google "deploy and access the kubernetes dashboard"

https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

Helm - is a package manager for K8s; similar to nodejs

1. install Helm --- helm.sh
https://helm.sh/docs/intro/install/   --- Through Package Managers
Install Chocolatey
cmd> choco install kubernetes-helm

cmd> helm version

# Add kubernetes-dashboard repository
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
# Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart
helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard


To access the Dashboard

cmd> kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443

 https://localhost:8443




==== Deep dive on Kubernetes YAML configurations to deploy a microservice ===========


So to get started with the deployment of our microservices, first, we need to give instructions to the Kubernetes on how we want to deploy our microservice and how we want to expose them.

So all these information we need to provide them in a Yaml configuration.

Whatever we have used with the help of Docker compose, we cannot simply give those to the Kubernetes because Kubernetes cannot understand the format or syntax of the Docker compose.

Kubernetes has its own format. We need to follow the same format and syntax and define how we want to deploy our microservices and how we want to expose them.

1. deploy config server

Google "Kubernetes Deployment"
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/


--- configserver.yml --

apiVersion: apps/v1
kind: Deployment
metadata:
  name: configserver-deployment
  labels:
    app: configserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: configserver
  template:
    metadata:
      labels:
        app: configserver
    spec:
      containers:
      - name: configserver
        image: 150478/configserver:s12
        ports:
        - containerPort: 8071
---
apiVersion: v1
kind: Service
metadata:
  name: configserver
spec:
  selector:
    app: configserver
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 8071
      targetPort: 8071






kind: Deployment --- is a predefined object inside the K8s 
replicas: 1   --- number of pods , based on the traffic needs we need to specify the number of replicas

---   --> consider it as 2 separate yamls

kind: Service  ---- to expose the deployed container to the outside world



==============================================
Deploying ConfigServer into Kubernetes Cluster
==============================================

cmd> kubectl get deployments

cmd> kubectl get services

cmd> kubectl get replicaset

cmd> kubectl apply -f configserver.yml

cmd> kubectl get deployments


